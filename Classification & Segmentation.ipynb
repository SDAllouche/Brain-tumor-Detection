{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a84b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import cv2\n",
    "import sys\n",
    "from skimage import io\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from IPython.display import display\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "%matplotlib inline\n",
    "import warnings #\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.morphology import label\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload dataframe\n",
    "brain_df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_df['Classe'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee715a",
   "metadata": {},
   "source": [
    "Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cf4e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "j=231\n",
    "for i in range(6):\n",
    "    I=plt.imread(brain_df.Path[i])\n",
    "    plt.subplot(j)\n",
    "    plt.imshow(I)\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583b72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data in mask column to string format, to use categorical mode in flow_from_dataframe\n",
    "# You will get this error message if you comment out the following code line:\n",
    "# TypeError: If class_mode=\"categorical\", y_col=\"mask\" column values must be type string, list or tuple.\n",
    "brain_df['Class'] = brain_df['Class'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf9801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(brain_df, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a546cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a image generator\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create a data generator which scales the data from 0 to 1 and makes validation split of 0.15\n",
    "datagen = ImageDataGenerator(rescale=1./255., validation_split = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator=datagen.flow_from_dataframe(\n",
    "dataframe=train,\n",
    "directory= './',\n",
    "x_col='Path',\n",
    "y_col='Class',\n",
    "subset=\"training\",\n",
    "batch_size=16,\n",
    "shuffle=True,\n",
    "class_mode=\"categorical\",\n",
    "target_size=(256,256))\n",
    "\n",
    "\n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "dataframe=train,\n",
    "directory= './',\n",
    "x_col='Path',\n",
    "y_col='Class',\n",
    "subset=\"validation\",\n",
    "batch_size=16,\n",
    "shuffle=True,\n",
    "class_mode=\"categorical\",\n",
    "target_size=(256,256))\n",
    "\n",
    "# Create a data generator for test images\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "dataframe=test,\n",
    "directory= './',\n",
    "x_col='Path',\n",
    "y_col='Class',\n",
    "batch_size=16,\n",
    "shuffle=False,\n",
    "class_mode='categorical',\n",
    "target_size=(256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7968f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ResNet50 base model\n",
    "basemodel = ResNet50(weights = 'imagenet', include_top = False, input_tensor = Input(shape=(256, 256, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basemodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f5ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the model weights\n",
    "\n",
    "for layer in basemodel.layers:\n",
    "  layers.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5162b6",
   "metadata": {},
   "source": [
    "Add layers (own touch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cfa9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add classification head to the base model\n",
    "headmodel = basemodel.output\n",
    "headmodel = AveragePooling2D(pool_size = (4,4))(headmodel)\n",
    "headmodel = Flatten(name= 'flatten')(headmodel)\n",
    "headmodel = Dense(256, activation = \"relu\")(headmodel)\n",
    "headmodel = Dropout(0.3)(headmodel)#\n",
    "headmodel = Dense(256, activation = \"relu\")(headmodel)\n",
    "headmodel = Dropout(0.3)(headmodel)\n",
    "#headmodel = Dense(256, activation = \"relu\")(headmodel)\n",
    "#headmodel = Dropout(0.3)(headmodel)\n",
    "headmodel = Dense(3, activation = 'softmax')(headmodel)\n",
    "\n",
    "model = Model(inputs = basemodel.input, outputs = headmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779159de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics= [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use early stopping to exit training if validation loss is not decreasing even after certain epochs (patience)\n",
    "earlystopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "\n",
    "# save the best model with least validation loss\n",
    "checkpointer = ModelCheckpoint(filepath=\"classifier-resnet-weights.hdf5\", verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad22382",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab87fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, steps_per_epoch= train_generator.n // 16,\n",
    "                    epochs = 20, validation_data= valid_generator,\n",
    "                    validation_steps= valid_generator.n // 16, callbacks=[checkpointer, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb975802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model architecture to json file for future use\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"classifier-resnet-model.json\",\"w\") as json_file:\n",
    "  json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb4d62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model (instead of training the model for 1+ hours) \n",
    "with open('classifier-resnet-model.json', 'r') as json_file:\n",
    "    json_savedModel= json_file.read()\n",
    "# load the model  \n",
    "model = tf.keras.models.model_from_json(json_savedModel)\n",
    "model.load_weights('classifier-resnet-weights.hdf5')\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics= [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "\n",
    "test_predict = model.predict(test_generator, steps = test_generator.n // 16, verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47947e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773bc0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the predicted class from the model prediction\n",
    "predict = []\n",
    "\n",
    "for i in test_predict:\n",
    "    predict.append(str(np.argmax(i)))\n",
    "\n",
    "predict = np.asarray(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e1ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have used test generator, it limited the images to len(predict), due to batch size\n",
    "original = np.asarray(test['Class'])[:len(predict)]\n",
    "len(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa30ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the accuracy of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(original, predict)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c98c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(original, predict)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e3dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(original, predict, labels = [0,1,2])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e82641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results.history['accuracy'])\n",
    "plt.plot(results.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "    \n",
    "# \"Loss\"\n",
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47521fb",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5190edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataframe containing MRIs which have masks associated with them.\n",
    "brain_df=pd.read_csv('data_mask.csv')\n",
    "brain_df_mask = brain_df[brain_df['mask'] == 1]\n",
    "brain_df_mask = brain_df_mask.drop(columns = ['patient_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a70377",
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_df_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c5e85c",
   "metadata": {},
   "source": [
    "Append paths in two lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aefb552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#liste of images path\n",
    "liste_path=[]\n",
    "#liste of masks path\n",
    "liste_mask=[]\n",
    "\n",
    "for i,j in enumerate(brain_df_mask.image_path):\n",
    "    a='Dataset/Mask/'+j\n",
    "    liste_path.append(a)\n",
    "\n",
    "for k,l in enumerate(brain_df_mask.mask_path):\n",
    "    b='Dataset/Mask/'+l\n",
    "    liste_mask.append(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c86376d",
   "metadata": {},
   "source": [
    "visualize images and thier masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab627b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    path=liste_path[i]\n",
    "    path1=liste_mask[i]\n",
    "    img = plt.imread(path)[:,:,:3]\n",
    "    img1 = plt.imread(path1)\n",
    "    f = plt.figure()\n",
    "    f.add_subplot(1,2, 1)\n",
    "    plt.imshow(np.rot90(img,2))\n",
    "    f.add_subplot(1,2, 2)\n",
    "    plt.imshow(np.rot90(img1,2))\n",
    "    plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e4c85d",
   "metadata": {},
   "source": [
    "Stock images in arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da4a83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from skimage.transform import resize\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_CHANNELS = 3\n",
    "print('Getting and resizing training images ... ')\n",
    "X_train = np.zeros((len(brain_df_mask), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "y_train=np.zeros((len(brain_df_mask), IMG_HEIGHT, IMG_WIDTH), dtype=np.bool)\n",
    "Y_train = np.zeros((len(brain_df_mask), IMG_HEIGHT, IMG_WIDTH,1), dtype=np.bool)\n",
    "        \n",
    "# Re-sizing our training images to 128 x 128\n",
    "# Note sys.stdout prints info that can be cleared unlike print.\n",
    "# Using TQDM allows us to create progress bars\n",
    "sys.stdout.flush()\n",
    "\n",
    "for i in range(len(brain_df_mask)):\n",
    "    path=liste_path[i]\n",
    "    path1=liste_mask[i]\n",
    "    img = plt.imread(path)[:,:,:IMG_CHANNELS]\n",
    "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    X_train[i] = img\n",
    "    mask = plt.imread(path1,0)\n",
    "    mask=resize(mask, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    y_train[i] = mask\n",
    "    Y_train[i]=y_train[i].reshape([128,128,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f06527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test\n",
    "X_test=X_train[1001:]\n",
    "X_train=X_train[:1000]\n",
    "Y_test=Y_train[1001:]\n",
    "Y_train=Y_train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810823b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If ther any eager errors\n",
    "#pip install tensorflow==2.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1af3d1",
   "metadata": {},
   "source": [
    "Build U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5068c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we make our layers varaibles so that we can concatenate or stack\n",
    "# This is required so that we can re-create our U-Net Model\n",
    "model = Sequential()\n",
    "model.call = tf.function(model.call)\n",
    "\n",
    "inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "s = Lambda(lambda x: x / 255) (inputs)\n",
    "\n",
    "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n",
    "p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n",
    "c2 = Dropout(0.1) (c2)\n",
    "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n",
    "p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n",
    "c3 = Dropout(0.2) (c3)\n",
    "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n",
    "p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n",
    "c4 = Dropout(0.2) (c4)\n",
    "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n",
    "p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\n",
    "c5 = Dropout(0.3) (c5)\n",
    "c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n",
    "\n",
    "u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "u6 = concatenate([u6, c4])\n",
    "c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n",
    "c6 = Dropout(0.2) (c6)\n",
    "c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n",
    "\n",
    "u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "u7 = concatenate([u7, c3])\n",
    "c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n",
    "c7 = Dropout(0.2) (c7)\n",
    "c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n",
    "\n",
    "u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "u8 = concatenate([u8, c2])\n",
    "c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n",
    "c8 = Dropout(0.1) (c8)\n",
    "c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n",
    "\n",
    "u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "u9 = concatenate([u9, c1], axis=3)\n",
    "c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\n",
    "c9 = Dropout(0.1) (c9)\n",
    "c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n",
    "\n",
    "# Note our output is effectively a mask of 128 x 128 \n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c957335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce18cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "model_path = \"nuclei_finder_unet_1.h5\"\n",
    "# Initialize our callbacks\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_path,\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 5,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8375ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model \n",
    "results = model.fit(X_train, Y_train, validation_split=0.1,\n",
    "                    batch_size=16, epochs=10,callbacks=[earlystop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ba1cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results.history['accuracy'])\n",
    "plt.plot(results.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "    \n",
    "# \"Loss\"\n",
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c01da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function of custom_objects\n",
    "def my_iou_metric(label, pred):\n",
    "    metric_value = tf.py_func(iou_metric_batch, [label, pred], tf.float32)     \n",
    "    return metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2861a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('nuclei_finder_unet_1.h5', \n",
    "                   custom_objects={'my_iou_metric': my_iou_metric})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656788cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the first 90% was used for training\n",
    "#preds_train = model.predict(X_train[:int(X_train.shape[0]*0.9)], verbose=1)\n",
    "\n",
    "\n",
    "preds_test = model.predict(X_test, verbose=1)\n",
    "\n",
    "# Threshold predictions\n",
    "preds_test_t = (preds_test > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6127d8e",
   "metadata": {},
   "source": [
    "visualise the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(10):\n",
    "    f=plt.figure(figsize=(15,15))\n",
    "    f.add_subplot(1,3, 1)\n",
    "    plt.imshow(np.rot90(X_test[n]))\n",
    "    f.add_subplot(1,3, 2)\n",
    "    plt.imshow(np.rot90(np.squeeze(preds_test_t[n])))\n",
    "    f.add_subplot(1,3, 3)\n",
    "    plt.imshow(np.rot90(np.squeeze(Y_test[n])))\n",
    "    plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50de1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
